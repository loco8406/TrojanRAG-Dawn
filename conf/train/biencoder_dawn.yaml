# @package _group_
# Dawn Cluster Biencoder Training Config
# Optimized for Intel PVC GPUs with distributed training

# Batch size per GPU (with 4 GPUs distributed, effective batch = 16 * 4 * 4 = 256)
batch_size: 16
dev_batch_size: 32

adam_eps: 1e-8
adam_betas: (0.9, 0.999)
max_grad_norm: 2.0
log_batch_step: 10
train_rolling_loss_step: 100
weight_decay: 0.0

# Learning rate - standard for BERT-based models
learning_rate: 2e-5

# Linear warmup over warmup_steps
warmup_steps: 1237

# Gradient accumulation for effective larger batch sizes
# With 4 Intel PVC GPUs: effective_batch = 16 * 4 * 4 = 256
gradient_accumulation_steps: 4

# Total number of training epochs
num_train_epochs: 40
eval_per_epoch: 1
hard_negatives: 1
other_negatives: 0
val_av_rank_hard_neg: 30
val_av_rank_other_neg: 30
val_av_rank_bsz: 128
val_av_rank_max_qs: 10000
